{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Lab: Language Understanding with Recurrent Networks\n",
    "\n",
    "**Если кто-то до этого не работал с Jupiter Notebooks, то можно почитать вот [здесь](http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/What%20is%20the%20Jupyter%20Notebook.html) **\n",
    "\n",
    "Данный туториал показывает применение рекурентной нейронной сети для обработки текста [Air Travel Information Services](https://catalog.ldc.upenn.edu/LDC95S26) \n",
    "(ATIS) задание по слот-тегированию (тегирование отдельных слов в необходимые классы, где классы описанны как \"лейблы\" в тренировочном наборе данных).\n",
    "\n",
    "Мы начнем с векторизации данных слов, продолжим их обработкой в рекурентной LSTM и расширим ее для включения соседних слов и параллельного запуска.\n",
    "\n",
    "\n",
    "Технологии, которые вы будете практиковать:\n",
    "\n",
    "* описание модели через составление блока слоев(layer block)-удобный способ для конструирования сетей без необходимости описывать математическую формулу     model description by composing layer blocks, a convenient way to compose \n",
    "  networks/models without requiring the need to write formulas,\n",
    "* создание создание блока слоев(layer block)     creating your own layer block\n",
    "* создание переменных с разной длинной последовательностей (sequence length) в одной нейросети   variables with different sequence lengths in the same network\n",
    "* тренировка нейросети training the network\n",
    "\n",
    "Предполагается знание основ глубокого обучения и следующих концептов:\n",
    "\n",
    "* рекурентные нейросети ([Wikipedia page](https://en.wikipedia.org/wiki/Recurrent_neural_network))\n",
    "* векторное представление текста ([Wikipedia page](https://en.wikipedia.org/wiki/Word_embedding))\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Мы предпологаем, что вы уже установили [CNTK](https://www.cntk.ai/pythondocs/setup.html).\n",
    "Данное руководство требует CNTK V2. Рекомендуется запускать данный туториал на устройстве с GPU(графический процессор), совместимом с технологией [CUDA](http://www.nvidia.ru/object/cuda-parallel-computing-ru.html). Глубокое обучение без GPU это не весело, хех."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Скачивание необходимых дата-сетов\n",
    "\n",
    "В данном руководстве мы будет использовать слегка переработанную версию датасета ATIS. Вы можете скачать его автоматически, запустив код в </i>клетке</i> ниже или вручную по ссылкам ниже.\n",
    "\n",
    "\n",
    "#### Инструкции для ручного скачивания\n",
    "Пожалуйста, скачайте блок данных ATIS для [тренировки](https://github.com/Microsoft/CNTK/blob/master/Tutorials/SLUHandsOn/atis.train.ctf) \n",
    "и [тест](https://github.com/Microsoft/CNTK/blob/master/Tutorials/SLUHandsOn/atis.test.ctf) \n",
    "поместите эти файлы в ту же папку, что и настоящий файл(notebook). Если вы хотите увидеть, как модель предугадывает слова на новых предложениях, которые вы вводите, вам так же понадобятся словари для запросов и слотов(queries and slots).\n",
    "[queries](https://github.com/Microsoft/CNTK/blob/master/Examples/Text/ATIS/query.wl) \n",
    "[slots](https://github.com/Microsoft/CNTK/blob/master/Examples/Text/ATIS/slots.wl) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download(url, filename):\n",
    "    \"\"\" utility to download necessary data \"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(filename, \"wb\") as handle:\n",
    "        for data in response.iter_content():\n",
    "            handle.write(data)\n",
    "\n",
    "url1 = \"https://github.com/Microsoft/CNTK/blob/master/Examples/Tutorials/SLUHandsOn/atis.%s.ctf?raw=true\"\n",
    "url2 = \"https://github.com/Microsoft/CNTK/blob/master/Examples/Text/ATIS/%s.wl?raw=true\"\n",
    "urls = [url1%\"train\", url1%\"test\", url2%\"query\", url2%\"slots\"]\n",
    "\n",
    "for t in urls:\n",
    "    filename = t.split('/')[-1].split('?')[0]\n",
    "    try:\n",
    "        f = open(filename)\n",
    "        f.close()\n",
    "    except IOError:\n",
    "        download(t, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Импорт CNTK и необходимых модулей\n",
    "\n",
    "CNTK-модуль Python, который содержит некоторые подмодули(`io`, `learner`,`layers`). Мы так же используем NumPy в некоторых случаях, так как результаты, возвращаемые CNTK работают как NumPy-массивы.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from cntk.blocks import default_options, LSTM, Placeholder, Input        # building blocks\n",
    "from cntk.layers import Embedding, Recurrence, Dense, BatchNormalization # layers\n",
    "from cntk.models import Sequential                                       # higher level things\n",
    "from cntk.utils import ProgressPrinter, log_number_of_parameters\n",
    "from cntk.io import MinibatchSource, CTFDeserializer\n",
    "from cntk.io import StreamDef, StreamDefs, INFINITELY_REPEAT, FULL_DATA_SWEEP\n",
    "from cntk import future_value, combine, Trainer, cross_entropy_with_softmax, classification_error, splice\n",
    "from cntk.learner import adam_sgd, learning_rate_schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание и структура модели\n",
    "\n",
    "Наша цель - присвоить каждое слово соответствующему слоту(slot tagging). \n",
    "Мы используем [ATIS corpus](https://catalog.ldc.upenn.edu/LDC95S26).\n",
    "ATIS содержит запросы от \"человека\" к \"компьютеру\" c домена Air Travel Information Services,\n",
    "и нашим заданием будет тэгировать каждое слово запроса к соответствующему слоту(единице информации) и определение к какому именно.\n",
    "\n",
    "Данные, скачанные нами в рабочую папку уже были конвертированы в \"CNTK Text Format\".\n",
    "Посмотрим на следующий тестовый файл `atis.test.ctf`:\n",
    "\n",
    "    19  |S0 178:1 |# BOS      |S1 14:1 |# flight  |S2 128:1 |# O\n",
    "    19  |S0 770:1 |# show                         |S2 128:1 |# O\n",
    "    19  |S0 429:1 |# flights                      |S2 128:1 |# O\n",
    "    19  |S0 444:1 |# from                         |S2 128:1 |# O\n",
    "    19  |S0 272:1 |# burbank                      |S2 48:1  |# B-fromloc.city_name\n",
    "    19  |S0 851:1 |# to                           |S2 128:1 |# O\n",
    "    19  |S0 789:1 |# st.                          |S2 78:1  |# B-toloc.city_name\n",
    "    19  |S0 564:1 |# louis                        |S2 125:1 |# I-toloc.city_name\n",
    "    19  |S0 654:1 |# on                           |S2 128:1 |# O\n",
    "    19  |S0 601:1 |# monday                       |S2 26:1  |# B-depart_date.day_name\n",
    "    19  |S0 179:1 |# EOS                          |S2 128:1 |# O\n",
    "\n",
    "Файл имеет 7 столбцов:\n",
    "\n",
    "* id последовательности (19). Всего 11 вхождений с этим id. Это значит, что эта последовательность состит из 11 слов(tokens);\n",
    "* столбец `S0`, который содержит индексы слов;\n",
    "* закомментированный `#` столбец, позволяющий читателю понять, что было введено;\n",
    "Закомментированные стобцы игнорируются системой. `BOS` и `EOS`-специальные слова, для обозначения начала и конца предложения;\n",
    "* столбец `S1` - специальный label, который мы будем использовать позже, в последеней части туториала;\n",
    "* другой закомментированный `#` столбец показывает маркировку(label) читаемую человеком;\n",
    "* столбец `S2` - маркировка нужного слота в виде индекса;\n",
    "* еще один столбец-комментарий, поясняющий label.\n",
    "\n",
    "Задание нейросети состоит в том, чтобы по последовательности(колонка S0) предсказать верную маркировку слота.\n",
    "\n",
    "Как вы можете видеть, каждому слову на входе присваивается либо пустой label `O`, либо специальный slot-label, начинающийся с   `B-` для первого слова, и с `I-` для каждого дополнительного слова последовательности принадлежащего этому же слоту. \n",
    "\n",
    "Мы будем использовать рекурентную модель, состоящую из трёх слоев: embedding(векторизация), рекурентную [LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) сеть и dense layer для рассчета последующих вероятностей.\n",
    "\n",
    "\n",
    "\n",
    "    slot label   \"O\"        \"O\"        \"O\"        \"O\"  \"B-fromloc.city_name\"\n",
    "                  ^          ^          ^          ^          ^\n",
    "                  |          |          |          |          |\n",
    "              +-------+  +-------+  +-------+  +-------+  +-------+\n",
    "              | Dense |  | Dense |  | Dense |  | Dense |  | Dense |  ...\n",
    "              +-------+  +-------+  +-------+  +-------+  +-------+\n",
    "                  ^          ^          ^          ^          ^\n",
    "                  |          |          |          |          |\n",
    "              +------+   +------+   +------+   +------+   +------+   \n",
    "         0 -->| LSTM |-->| LSTM |-->| LSTM |-->| LSTM |-->| LSTM |-->...\n",
    "              +------+   +------+   +------+   +------+   +------+   \n",
    "                  ^          ^          ^          ^          ^\n",
    "                  |          |          |          |          |\n",
    "              +-------+  +-------+  +-------+  +-------+  +-------+\n",
    "              | Embed |  | Embed |  | Embed |  | Embed |  | Embed |  ...\n",
    "              +-------+  +-------+  +-------+  +-------+  +-------+\n",
    "                  ^          ^          ^          ^          ^\n",
    "                  |          |          |          |          |\n",
    "    w      ------>+--------->+--------->+--------->+--------->+------... \n",
    "                 BOS      \"show\"    \"flights\"    \"from\"   \"burbank\"\n",
    "\n",
    "Посмотрите на описание сети в языке CNTK и сравните со схемой, представленной выше:\n",
    "(описания функций [здесь](http://cntk.ai/pythondocs/layerref.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of words in vocab, slot labels, and intent labels\n",
    "vocab_size = 943 ; num_labels = 129 ; num_intents = 26    \n",
    "\n",
    "# model dimensions\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 150\n",
    "hidden_dim = 300\n",
    "\n",
    "def create_model():\n",
    "    with default_options(initial_state=0.1):\n",
    "        return Sequential([\n",
    "            Embedding(emb_dim),\n",
    "            Recurrence(LSTM(hidden_dim), go_backwards=False),\n",
    "            Dense(num_labels)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы готовы к созданию и осмотру модели.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(-1, 150)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# peek\n",
    "model = create_model()\n",
    "print(len(model.layers))\n",
    "print(model.layers[0].E.shape)\n",
    "print(model.layers[2].b.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Как вы можете видеть, атрибуты модели полностью доступны через Python. Модель имеет 3 слоя. Первый слой-векторизация/отображение(embedding), доступ к нему осуществляется через параметр `E` (где хранятся embeddings) аналогично любому другому свойству объекта Python. Его свойство .shape содерджит `-1`, что указывает на то, что данный параметр еще не полностью определен. Когда мы решим, какие данные мы будет пропускать через данную сеть, .shape примет значение равное размеру словаря на висходных данных. Мы так же напечатали bias term(что-то вроде погрешности выборки, подробней [здесь](https://www.quora.com/What-does-the-bias-term-represent-in-logistic-regression) и [здесь](http://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks/2499936#2499936)) для последнего слоя. Bias terms изначально установлены на 0(но мы можем изменить их).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNTK-конфигурация\n",
    "\n",
    "Для тренировки и тестирования модели в CNTK мы должны создать эту модели и провести уточнения по чтению даты и проведению тренировок и тестов.\n",
    "\n",
    "Для того, чтобы тренировать нашу сеть мы должны определить:\n",
    "\n",
    "* как читать данные \n",
    "* функции модели, данные на входе и выходе\n",
    "* специальные параметры для \"ученика\", такие как статистика обучения\n",
    "\n",
    "[comment]: <> (For testing ...)\n",
    "\n",
    "### О данных и об их чтении \n",
    "\n",
    "Мы уже посмотрели на данные.\n",
    "Однако как мы можем сгенерировать их в нужном формате?\n",
    "\n",
    "Для чтения текста мы будем использовать `CNTKTextFormatReader`. Данный модуль ожидает входные данные в специальном формате, описанном [здесь](https://github.com/Microsoft/CNTK/wiki/CNTKTextFormat-Reader).\n",
    "\n",
    "Для данного руководства необходимо совершить следующее:\n",
    "* convert the raw data into a plain text file that contains of TAB-separated столбецs of space-separated text. For example:\n",
    "\n",
    "  ```\n",
    "  BOS show flights from burbank to st. louis on monday EOS (TAB) flight (TAB) O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name O B-depart_date.day_name O\n",
    "  ```\n",
    "\n",
    "Это должно быть совместимо с выводом команды `paste`.\n",
    "* конвертируем в специальный текстовый формат CNTK (CTF) с помощью следующей команды:\n",
    "\n",
    "  ```\n",
    "  python [CNTK root]/Scripts/txt2ctf.py --map query.wl intent.wl slots.wl --annotated True --input atis.test.txt --output    atis.test.ctf\n",
    "  ```\n",
    "  \n",
    "  \n",
    "  где три `.wl` файла дают словарь в виде простых текстовых файлов, по одному слову в строке\n",
    "  \n",
    "В этих CTF-файлах, наши столбцы уже промаркированы `S0`, `S1`, and `S2`.\n",
    "Они присоединены к \"входам\"(inputs) нашей сети через следующие строки кода определяющих \"читателя\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reader(path, is_training):\n",
    "    return MinibatchSource(CTFDeserializer(path, StreamDefs(\n",
    "         query         = StreamDef(field='S0', shape=vocab_size,  is_sparse=True),\n",
    "         intent_unused = StreamDef(field='S1', shape=num_intents, is_sparse=True),  \n",
    "         slot_labels   = StreamDef(field='S2', shape=num_labels,  is_sparse=True)\n",
    "     )), randomize=is_training, epoch_size = INFINITELY_REPEAT if is_training else FULL_DATA_SWEEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# peek\n",
    "reader = create_reader(\"atis.train.ctf\", is_training=True)\n",
    "reader.streams.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренировка\n",
    "\n",
    "Мы также должны определить критерий тренировки, в роли которого будет выступать [функция потерь](https://en.wikipedia.org/wiki/Loss_function), а так же метрику ошибок для отслеживания. В коде ниже мы обширно используем `Placeholders`. Важно помнить, что код, который мы писали до этого не выполняет никаких тяжелых вычислений. Он лишь определяет функцию, которую мы хотим использовать на данных во время тренировки и тестирования. Так же, как удобно иметь имена для аргументов при написании обычной функции, удобно иметь `Placeholders`, которые ссылаются на аргументы (или локальные вычисления, которые нужно повторить). В какой то момент, другой код заменить `Placeholders` другими известными величинами, так же как обычная функция вызывается с определенными величинами, привязанными к ее аргументам.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_criterion_function(model):\n",
    "    labels = Placeholder()\n",
    "    ce   = cross_entropy_with_softmax(model, labels)\n",
    "    errs = classification_error      (model, labels)\n",
    "    return combine ([ce, errs]) # (features, labels) -> (loss, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(reader, model, max_epochs=16):\n",
    "    # criterion: (model args, labels) -> (loss, metric)\n",
    "    #   here  (query, slot_labels) -> (ce, errs)\n",
    "    criterion = create_criterion_function(model)\n",
    "\n",
    "    criterion.replace_placeholders({criterion.placeholders[0]: Input(vocab_size),\n",
    "                                    criterion.placeholders[1]: Input(num_labels)})\n",
    "\n",
    "    # training config\n",
    "    epoch_size = 18000        # 18000 samples is half the dataset size \n",
    "    minibatch_size = 70\n",
    "    \n",
    "    # LR schedule over epochs \n",
    "    # In CNTK, an epoch is how often we get out of the minibatch loop to\n",
    "    # do other stuff (e.g. checkpointing, adjust learning rate, etc.)\n",
    "    # (we don't run this many epochs, but if we did, these are good values)\n",
    "    lr_per_sample = [0.003]*4+[0.0015]*24+[0.0003]\n",
    "    lr_schedule = learning_rate_schedule(lr_per_sample, units=epoch_size)\n",
    "    \n",
    "    # Momentum (could also be on a schedule)\n",
    "    momentum_as_time_constant = 700\n",
    "    \n",
    "    # We use a variant of the Adam optimizer which is known to work well on this dataset\n",
    "    # Feel free to try other optimizers from \n",
    "    # https://www.cntk.ai/pythondocs/cntk.learner.html#module-cntk.learner\n",
    "    learner = adam_sgd(criterion.parameters,\n",
    "                       lr_per_sample=lr_schedule, momentum_time_constant=momentum_as_time_constant,\n",
    "                       low_memory=True,\n",
    "                       gradient_clipping_threshold_per_sample=15, gradient_clipping_with_truncation=True)\n",
    "\n",
    "    # trainer\n",
    "    trainer = Trainer(model, criterion.outputs[0], criterion.outputs[1], learner)\n",
    "\n",
    "    # process minibatches and perform model training\n",
    "    log_number_of_parameters(model)\n",
    "    progress_printer = ProgressPrinter(tag='Training')\n",
    "    #progress_printer = ProgressPrinter(freq=100, first=10, tag='Training') # more detailed logging\n",
    "\n",
    "    t = 0\n",
    "    for epoch in range(max_epochs):         # loop over epochs\n",
    "        epoch_end = (epoch+1) * epoch_size\n",
    "        while t < epoch_end:                # loop over minibatches on the epoch\n",
    "            data = reader.next_minibatch(minibatch_size, input_map={  # fetch minibatch\n",
    "                criterion.arguments[0]: reader.streams.query,\n",
    "                criterion.arguments[1]: reader.streams.slot_labels\n",
    "            })\n",
    "            trainer.train_minibatch(data)                                     # update model with it\n",
    "            t += data[criterion.arguments[1]].num_samples                     # samples so far\n",
    "            progress_printer.update_with_trainer(trainer, with_metric=True)   # log progress\n",
    "        loss, metric, actual_samples = progress_printer.epoch_summary(with_metric=True)\n",
    "\n",
    "    return loss, metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запуск\n",
    "\n",
    "Так выглядит готовая модель запуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def do_train():\n",
    "    global model\n",
    "    model = create_model()\n",
    "    reader = create_reader(\"atis.train.ctf\", is_training=True)\n",
    "    train(reader, model)\n",
    "do_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Код выше показывает, как процесс обучения проходит через данные.\n",
    "Например, после четрых циклов(epoch) потери достигли 0.22, как было измерено на ~18000 образцах этой выборки, а ошибки возникли в 5% случаев на данных образцах.\n",
    "\n",
    "\n",
    "Размер выборки-количество образцов, то есть слов, а не предложений, которые были обработаны моделью.\n",
    "\n",
    "The epoch size is the number of samples--counted as *word tokens*, not sentences--to\n",
    "process between model checkpoints.\n",
    "\n",
    "Когда тренировка будет выполнена, вы увидите следующее:\n",
    "```\n",
    "Finished Epoch [16]: [Training] loss = 0.058111 * 18014, metric = 1.3% * 18014\n",
    "```\n",
    ", где loss (cross entropy)-потери, а metric-ошибка классификации посчитаны в среднем, после последнего \"прохода\".\n",
    "\n",
    "На компьютере оснащенном только CPU, тренировка может занять в 4 раза дольше. Вы можете попробовать установить\n",
    "```\n",
    "python\n",
    "emb_dim    = 50 \n",
    "hidden_dim = 100\n",
    "```\n",
    "чтобы уменьшить время обработки, однако модель может не подойти под эти критерии, так же как в тех случаях, когда слои будут больше. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка модели\n",
    "\n",
    "Так же как и функцию train(), мы должны объявить функцию для измерения \"аккуратности\" нейросети на тестовом сете данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(reader, model):\n",
    "    criterion = create_criterion_function(model)\n",
    "    criterion.replace_placeholders({criterion.placeholders[0]: Input(num_labels)})\n",
    "\n",
    "    # process minibatches and perform evaluation\n",
    "    dummy_learner = adam_sgd(criterion.parameters, \n",
    "                             lr_per_sample=1, momentum_time_constant=0, low_memory=True)\n",
    "    evaluator = Trainer(model, criterion.outputs[0], criterion.outputs[1], dummy_learner)\n",
    "    progress_printer = ProgressPrinter(tag='Evaluation')\n",
    "\n",
    "    while True:\n",
    "        minibatch_size = 1000\n",
    "        data = reader.next_minibatch(minibatch_size, input_map={  # fetch minibatch\n",
    "            criterion.arguments[0]: reader.streams.query,\n",
    "            criterion.arguments[1]: reader.streams.slot_labels\n",
    "        })\n",
    "        if not data:                                 # until we hit the end\n",
    "            break\n",
    "        metric = evaluator.test_minibatch(data)\n",
    "        progress_printer.update(0, data[criterion.arguments[1]].num_samples, metric) # log progress\n",
    "    loss, metric, actual_samples = progress_printer.epoch_summary(with_metric=True)\n",
    "\n",
    "    return loss, metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем измерить аккуратность модели, проходя через все примеры тестового наобра и используя метод ``test_minibatch``, который создан внутри функции ``evaluate``, определенной выше. На данный момент, конструктору Trainer необходим ученик(даже если он используется только для запуска ``test_minibatch``), поэтому необходимо \"уточнить\" какого-нибудь глупенького ученика:)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_test():\n",
    "    reader = create_reader(\"atis.test.ctf\", is_training=False)\n",
    "    evaluate(reader, model)\n",
    "do_test()\n",
    "model.layers[2].b.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load dictionaries\n",
    "query_wl = [line.rstrip('\\n') for line in open('query.wl')]\n",
    "slots_wl = [line.rstrip('\\n') for line in open('slots.wl')]\n",
    "query_dict = {query_wl[i]:i for i in range(len(query_wl))}\n",
    "slots_dict = {slots_wl[i]:i for i in range(len(slots_wl))}\n",
    "\n",
    "# let's run a sequence through\n",
    "seq = 'BOS flights from new york to seattle EOS'\n",
    "w = [query_dict[w] for w in seq.split()] # convert to word indices\n",
    "print(w)\n",
    "onehot = np.zeros([len(w),len(query_dict)], np.float32)\n",
    "for t in range(len(w)):\n",
    "    onehot[t,w[t]] = 1\n",
    "pred = model.eval({model.arguments[0]:onehot})\n",
    "print(pred.shape)\n",
    "best = np.argmax(pred,axis=2)\n",
    "print(best[0])\n",
    "list(zip(seq.split(),[slots_wl[s] for s in best[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the Model\n",
    "\n",
    "In the following, you will be given tasks to practice modifying CNTK configurations.\n",
    "The solutions are given at the end of this document... but please try without!\n",
    "\n",
    "### A Word About [`Sequential()`](https://www.cntk.ai/pythondocs/layerref.html#sequential)\n",
    "\n",
    "Before jumping to the tasks, let's have a look again at the model we just ran.\n",
    "The model is described in what we call *function-composition style*.\n",
    "```python\n",
    "        Sequential([\n",
    "            Embedding(emb_dim),\n",
    "            Recurrence(LSTM(hidden_dim), go_backwards=False),\n",
    "            Dense(num_labels)\n",
    "        ])\n",
    "```\n",
    "You may be familiar with the \"sequential\" notation from other neural-network toolkits.\n",
    "If not, [`Sequential()`](https://www.cntk.ai/pythondocs/layerref.html#sequential) is a powerful operation that,\n",
    "in a nutshell, allows to compactly express a very common situation in neural networks\n",
    "where an input is processed by propagating it through a progression of layers.\n",
    "`Sequential()` takes an list of functions as its argument,\n",
    "and returns a *new* function that invokes these functions in order,\n",
    "each time passing the output of one to the next.\n",
    "For example,\n",
    "```python\n",
    "\tFGH = Sequential ([F,G,H])\n",
    "    y = FGH (x)\n",
    "```\n",
    "means the same as\n",
    "```\n",
    "    y = H(G(F(x))) \n",
    "```\n",
    "This is known as [\"function composition\"](https://en.wikipedia.org/wiki/Function_composition),\n",
    "and is especially convenient for expressing neural networks, which often have this form:\n",
    "\n",
    "         +-------+   +-------+   +-------+\n",
    "    x -->|   F   |-->|   G   |-->|   H   |--> y\n",
    "         +-------+   +-------+   +-------+\n",
    "\n",
    "Coming back to our model at hand, the `Sequential` expression simply\n",
    "says that our model has this form:\n",
    "\n",
    "         +-----------+   +----------------+   +------------+\n",
    "    x -->| Embedding |-->| Recurrent LSTM |-->| DenseLayer |--> y\n",
    "         +-----------+   +----------------+   +------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Add Batch Normalization\n",
    "\n",
    "We now want to add new layers to the model, specifically batch normalization.\n",
    "\n",
    "Batch normalization is a popular technique for speeding up convergence.\n",
    "It is often used for image-processing setups, for example our other [hands-on lab on image\n",
    "recognition](./Hands-On-Labs-Image-Recognition).\n",
    "But could it work for recurrent models, too?\n",
    "  \n",
    "So your task will be to insert batch-normalization layers before and after the recurrent LSTM layer.\n",
    "If you have completed the [hands-on labs on image processing](https://github.com/Microsoft/CNTK/blob/master/bindings/python/tutorials/CNTK_201B_CIFAR-10_ImageHandsOn.ipynb),\n",
    "you may remember that the [batch-normalization layer](https://www.cntk.ai/pythondocs/layerref.html#batchnormalization-layernormalization-stabilizer) has this form:\n",
    "```\n",
    "    BatchNormalization()\n",
    "```\n",
    "So please go ahead and modify the configuration and see what happens.\n",
    "\n",
    "If everything went right, you will notice improved convergence speed (`loss` and `metric`)\n",
    "compared to the previous configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your task: Add batch normalization\n",
    "def create_model():\n",
    "    with default_options(initial_state=0.1):\n",
    "        return Sequential([\n",
    "            Embedding(emb_dim),\n",
    "            Recurrence(LSTM(hidden_dim), go_backwards=False),\n",
    "            Dense(num_labels)\n",
    "        ])\n",
    "\n",
    "do_train()\n",
    "do_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Add a Lookahead \n",
    "\n",
    "Our recurrent model suffers from a structural deficit:\n",
    "Since the recurrence runs from left to right, the decision for a slot label\n",
    "has no information about upcoming words. The model is a bit lopsided.\n",
    "Your task will be to modify the model such that\n",
    "the input to the recurrence consists not only of the current word, but also of the next one\n",
    "(lookahead).\n",
    "\n",
    "Your solution should be in function-composition style.\n",
    "Hence, you will need to write a Python function that does the following:\n",
    "\n",
    "* takes no input arguments\n",
    "* creates a placeholder (sequence) variable\n",
    "* computes the \"next value\" in this sequence using the `future_value()` operation and\n",
    "* concatenates the current and the next value into a vector of twice the embedding dimension using `splice()`\n",
    "\n",
    "and then insert this function into `Sequential()`'s list right after the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your task: Add lookahead\n",
    "def create_model():\n",
    "    with default_options(initial_state=0.1):\n",
    "        return Sequential([\n",
    "            Embedding(emb_dim),\n",
    "            Recurrence(LSTM(hidden_dim), go_backwards=False),\n",
    "            Dense(num_labels)\n",
    "        ])\n",
    "do_train()\n",
    "do_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Bidirectional Recurrent Model\n",
    "\n",
    "Aha, knowledge of future words help. So instead of a one-word lookahead,\n",
    "why not look ahead until all the way to the end of the sentence, through a backward recurrence?\n",
    "Let us create a bidirectional model!\n",
    "\n",
    "Your task is to implement a new layer that\n",
    "performs both a forward and a backward recursion over the data, and\n",
    "concatenates the output vectors.\n",
    "\n",
    "Note, however, that this differs from the previous task in that\n",
    "the bidirectional layer contains learnable model parameters.\n",
    "In function-composition style,\n",
    "the pattern to implement a layer with model parameters is to write a *factory function*\n",
    "that creates a *function object*.\n",
    "\n",
    "A function object, also known as [*functor*](https://en.wikipedia.org/wiki/Function_object), is an object that is both a function and an object.\n",
    "Which means nothing else that it contains data yet still can be invoked as if it was a function.\n",
    "\n",
    "For example, `Dense(outDim)` is a factory function that returns a function object that contains\n",
    "a weight matrix `W`, a bias `b`, and another function to compute \n",
    "`input @ W + b.` (This is using \n",
    "[Python 3.5 notation for matrix multiplication](https://docs.python.org/3/whatsnew/3.5.html#whatsnew-pep-465).\n",
    "In Numpy syntax it is `input.dot(W) + b`).\n",
    "E.g. saying `Dense(1024)` will create this function object, which can then be used\n",
    "like any other function, also immediately: `Dense(1024)(x)`. \n",
    "\n",
    "Let's look at an example for further clarity: Let us implement a new layer that combines\n",
    "a linear layer with a subsequent batch normalization. \n",
    "To allow function composition, the layer needs to be realized as a factory function,\n",
    "which could look like this:\n",
    "\n",
    "```python\n",
    "def DenseLayerWithBN(dim):\n",
    "    F = Dense(dim)\n",
    "    G = BatchNormalization()\n",
    "    x = Placeholder()\n",
    "    apply_x = G(F(x))\n",
    "    return apply_x\n",
    "```\n",
    "\n",
    "Invoking this factory function will create `F`, `G`, `x`, and `apply_x`. In this example, `F` and `G` are function objects themselves, and `apply_x` is the function to be applied to the data.\n",
    "Thus, e.g. calling `DenseLayerWithBN(1024)` will\n",
    "create an object containing a linear-layer function object called `F`, a batch-normalization function object `G`,\n",
    "and `apply_x` which is the function that implements the actual operation of this layer\n",
    "using `F` and `G`. It will then return `apply_x`. To the outside, `apply_x` looks and behaves\n",
    "like a function. Under the hood, however, `apply_x` retains access to its specific instances of `F` and `G`.\n",
    "\n",
    "Now back to our task at hand. You will now need to create a factory function,\n",
    "very much like the example above.\n",
    "You shall create a factory function\n",
    "that creates two recurrent layer instances (one forward, one backward), and then defines an `apply_x` function\n",
    "which applies both layer instances to the same `x` and concatenate the two results.\n",
    "\n",
    "Allright, give it a try! To know how to realize a backward recursion in CNTK,\n",
    "please take a hint from how the forward recursion is done.\n",
    "Please also do the following:\n",
    "* remove the one-word lookahead you added in the previous task, which we aim to replace; and\n",
    "* make sure each LSTM is using `hidden_dim//2` outputs to keep the total number of model parameters limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your task: Add bidirectional recurrence\n",
    "def create_model():\n",
    "    with default_options(initial_state=0.1):  \n",
    "        return Sequential([\n",
    "            Embedding(emb_dim),\n",
    "            Recurrence(LSTM(hidden_dim), go_backwards=False),\n",
    "            Dense(num_labels)\n",
    "        ])\n",
    "do_train()\n",
    "do_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works like a charm! This model achieves 2.1%, a tiny bit better than the lookahead model above.\n",
    "The bidirectional model has 40% less parameters than the lookahead one. However, if you go back and look closely\n",
    "you may find that the lookahead one trained about 30% faster.\n",
    "This is because the lookahead model has both less horizontal dependencies (one instead of two\n",
    "recurrences) and larger matrix products, and can thus achieve higher parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1: Adding Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    with default_options(initial_state=0.1):\n",
    "        return Sequential([\n",
    "            Embedding(emb_dim),\n",
    "            BatchNormalization(),\n",
    "            Recurrence(LSTM(hidden_dim), go_backwards=False),\n",
    "            BatchNormalization(),\n",
    "            Dense(num_labels)\n",
    "        ])\n",
    "\n",
    "do_train()\n",
    "do_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: Add a Lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def OneWordLookahead():\n",
    "    x = Placeholder()\n",
    "    apply_x = splice ([x, future_value(x)])\n",
    "    return apply_x\n",
    "\n",
    "def create_model():\n",
    "    with default_options(initial_state=0.1):\n",
    "        return Sequential([\n",
    "            Embedding(emb_dim),\n",
    "            OneWordLookahead(),\n",
    "            Recurrence(LSTM(hidden_dim), go_backwards=False),\n",
    "            Dense(num_labels)        \n",
    "        ])\n",
    "\n",
    "do_train()\n",
    "do_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3: Bidirectional Recurrent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def BiRecurrence(fwd, bwd):\n",
    "    F = Recurrence(fwd)\n",
    "    G = Recurrence(bwd, go_backwards=True)\n",
    "    x = Placeholder()\n",
    "    apply_x = splice ([F(x), G(x)])\n",
    "    return apply_x \n",
    "\n",
    "def create_model():\n",
    "    with default_options(initial_state=0.1):\n",
    "        return Sequential([\n",
    "            Embedding(emb_dim),\n",
    "            BiRecurrence(LSTM(hidden_dim//2), LSTM(hidden_dim//2)),\n",
    "            Dense(num_labels)\n",
    "        ])\n",
    "\n",
    "do_train()\n",
    "do_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [cntk-py34]",
   "language": "python",
   "name": "Python [cntk-py34]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
